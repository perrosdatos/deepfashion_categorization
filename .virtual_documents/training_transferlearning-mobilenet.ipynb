import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = ALL, 1 = WARNING, 2 = ERROR, 3 = FATAL





accepted_categories = ["dress", "high_heel", "handbag", "skirt", "outerwear", "boot"]
label2id = {
    "dress": 0,
    "high_heel": 1, 
    "handbag": 2,
    "skirt": 3, 
    "outerwear": 4, 
    "boot": 5
    
}
id2label = {
    0: "dress",
    1: "high_heel", 
    2: "handbag",
    3: "skirt", 
    4: "outerwear", 
    5: "boot"
}

RANDOM_STATE = 12345





import numpy as np
import os
import pandas as pd
import tensorflow as tf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_and_save_metrics(model, val_ds, id2label, output_folder):
    """
    Evaluates accuracy, top-2, top-3 accuracy, precision, recall, and F1-score.
    Prints the results and saves them to an Excel file.

    Parameters:
    - model: Trained Keras model
    - val_ds: Validation tf.data.Dataset (batched)
    - id2label: Dictionary mapping label IDs to class names
    - output_folder: Folder path to save the Excel file
    """
    y_true = []
    y_pred = []
    y_pred_probs = []

    # Collect predictions
    for images, labels in val_ds:
        probs = model.predict(images, verbose=0)
        preds = tf.argmax(probs, axis=1).numpy()
        y_true.extend(labels.numpy())
        y_pred.extend(preds)
        y_pred_probs.extend(probs)

    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    y_pred_probs = np.array(y_pred_probs)

    # Top-k accuracy
    acc = accuracy_score(y_true, y_pred)
    top_2_acc = tf.keras.metrics.top_k_categorical_accuracy(
        tf.one_hot(y_true, depth=len(id2label)), y_pred_probs, k=2
    ).numpy().mean()

    top_3_acc = tf.keras.metrics.top_k_categorical_accuracy(
        tf.one_hot(y_true, depth=len(id2label)), y_pred_probs, k=3
    ).numpy().mean()

    # Class-wise metrics
    precision = precision_score(y_true, y_pred, average=None, zero_division=0)
    recall = recall_score(y_true, y_pred, average=None, zero_division=0)
    f1 = f1_score(y_true, y_pred, average=None, zero_division=0)

    # Print summary
    print(f"\n✅ Accuracy:     {acc:.4f}")
    print(f"🎯 Top-2 Acc:    {top_2_acc:.4f}")
    print(f"🎯 Top-3 Acc:    {top_3_acc:.4f}")
    print("\n📊 Per-class metrics:\n")

    rows = []
    for idx, class_name in id2label.items():
        row = {
            "class_id": idx,
            "class_name": class_name,
            "precision": precision[idx],
            "recall": recall[idx],
            "f1_score": f1[idx]
        }
        print(f"{class_name:15}  Precision: {precision[idx]:.4f}  Recall: {recall[idx]:.4f}  F1: {f1[idx]:.4f}")
        rows.append(row)

    # Save to Excel
    df_metrics = pd.DataFrame(rows)
    os.makedirs(output_folder, exist_ok=True)
    output_path = os.path.join(output_folder, "classification_report.xlsx")
    df_metrics.to_excel(output_path, index=False)
    print(f"\n📁 Report saved to: {output_path}")



import pandas as pd
import matplotlib.pyplot as plt
import random
from PIL import Image

def plot_random_images_per_category(df, image_column='path', category_column='global_category', max_per_row=4):
    """
    Plot one random image per category from the dataframe.

    Parameters:
    - df: pandas.DataFrame with image paths and categories.
    - image_column: str, column name with image file paths.
    - category_column: str, column name with image categories.
    - max_per_row: int, number of images per row in the plot grid.
    """
    categories = df[category_column].unique()
    sampled_rows = df.groupby(category_column).apply(lambda x: x.sample(1)).reset_index(drop=True)

    num_categories = len(categories)
    num_cols = min(max_per_row, num_categories)
    num_rows = (num_categories + num_cols - 1) // num_cols

    fig, axes = plt.subplots(num_rows, num_cols, figsize=(4 * num_cols, 4 * num_rows))
    axes = axes.flatten() if num_categories > 1 else [axes]

    for ax, (_, row) in zip(axes, sampled_rows.iterrows()):
        try:
            image = Image.open(row[image_column])
            ax.imshow(image)
            ax.set_title(f"{row[category_column]}")
            ax.axis('off')
        except Exception as e:
            ax.text(0.5, 0.5, "Error loading image", ha='center')
            ax.set_title(row[category_column])
            ax.axis('off')
            print(f"Failed to load image at {row[image_column]}: {e}")

    # Hide unused subplots
    for ax in axes[len(sampled_rows):]:
        ax.axis('off')

    plt.tight_layout()
    plt.show()



import pandas as pd

def balance_dataframe_by_class(df, category_column='global_category', samples_per_class=100, random_state=42):
    """
    Returns a balanced dataframe with at most `samples_per_class` samples per category.
    If a category has fewer than the desired amount, all its samples are included.

    Parameters:
    - df (pd.DataFrame): Original DataFrame with category information.
    - category_column (str): Column name to balance by (e.g., 'global_category').
    - samples_per_class (int): Target number of samples per class.
    - random_state (int): Seed for reproducible sampling.

    Returns:
    - pd.DataFrame: Balanced DataFrame.
    """
    balanced_dfs = []

    for category, group in df.groupby(category_column):
        if len(group) <= samples_per_class:
            balanced_dfs.append(group)
        else:
            balanced_dfs.append(group.sample(n=samples_per_class, random_state=random_state))

    balanced_df = pd.concat(balanced_dfs).reset_index(drop=True)
    return balanced_df



import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

def plot_top3_predictions(model, val_ds, id2label, rows=3, cols=8):
    """
    Muestra imágenes con las Top-3 predicciones de un modelo, coloreando en verde
    si la etiqueta verdadera está entre ellas, y rojo en caso contrario.

    Parameters:
    - model: modelo Keras o TensorFlow.
    - val_ds: dataset iterable (como tf.data.Dataset).
    - id2label: diccionario {id: nombre_clase}.
    - rows: número de filas a mostrar.
    - cols: número de columnas a mostrar.
    """
    val_images, val_labels = next(iter(val_ds))
    pred_probs = model.predict(val_images)
    
    top3_pred_indices = tf.math.top_k(pred_probs, k=3).indices.numpy()

    fig, axes = plt.subplots(rows, cols, figsize=(20, 10))
    fig.suptitle("Top-3 Predicciones del modelo", fontsize=16)

    for i, ax in enumerate(axes.flat):
        if i >= len(val_images):
            break

        image = val_images[i].numpy()
        true_label = int(val_labels[i].numpy())
        top3_labels = top3_pred_indices[i]

        top3_names = [id2label[idx] for idx in top3_labels]
        pred_text = "\n".join([f"{j+1}: {name}" for j, name in enumerate(top3_names)])

        color = "green" if true_label in top3_labels else "red"

        ax.imshow(image)
        ax.set_title(f"{pred_text}\nT: {id2label[true_label]}", color=color, fontsize=10)
        ax.axis("off")

    plt.tight_layout()
    plt.subplots_adjust(top=0.88)
    plt.show()






import pandas as pd

train_dataframe = balance_dataframe_by_class(pd.read_csv("conf/train.csv"), samples_per_class=600, random_state=RANDOM_STATE)
validation_dataframe = balance_dataframe_by_class(pd.read_csv("conf/validation.csv"), samples_per_class=100, random_state=RANDOM_STATE)


plot_random_images_per_category(train_dataframe)


plot_random_images_per_category(validation_dataframe)


train_dataframe.groupby("global_category").size()


validation_dataframe.groupby("label_id").size()


train_dataframe.head(3)








import tensorflow as tf

IMG_SIZE = 224
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

def decode_image(filename, label):
    image = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    image = image / 255.0  # Normalizar
    return image, label

def prepare_dataset(df, shuffle=True):
    paths = df["path"].values
    labels = df["label_id"].values
    ds = tf.data.Dataset.from_tensor_slices((paths, labels))
    ds = ds.map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        ds = ds.shuffle(buffer_size=20_000)  # optimize memory
    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    #ds = ds.repeat()
    ds = ds.apply(tf.data.experimental.ignore_errors())
    return ds

train_ds = prepare_dataset(train_dataframe)
val_ds = prepare_dataset(validation_dataframe, shuffle=True)





from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras import layers, models

def sparse_top_k_categorical_accuracy(y_true, y_pred, k=4):
    # Convierte etiquetas de enteros a one-hot para que funcione con top_k
    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])
    return top_k_categorical_accuracy(y_true, y_pred, k=k)





import tensorflow as tf
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.metrics import top_k_categorical_accuracy

# Custom sparse top-k accuracy metric
def sparse_top_k_categorical_accuracy(y_true, y_pred, k=4):
    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[-1])
    return top_k_categorical_accuracy(y_true, y_pred, k=k)

# Define top-2 and top-3 metrics
top_2_accuracy = lambda y_true, y_pred: sparse_top_k_categorical_accuracy(y_true, y_pred, k=2)
top_3_accuracy = lambda y_true, y_pred: sparse_top_k_categorical_accuracy(y_true, y_pred, k=3)

# Load base MobileNet with ImageNet weights and without top
base_model = MobileNet(
    include_top=False,
    weights="imagenet",
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

# Add custom classification head
x = GlobalAveragePooling2D()(base_model.output)
x = Dropout(0.2)(x)
output = Dense(len(accepted_categories), activation="softmax")(x)

# Final model
model_mobilenet = Model(inputs=base_model.input, outputs=output)

# Freeze all layers initially
for layer in base_model.layers:
    layer.trainable = False

# Unfreeze last 18 layers
for layer in base_model.layers[-9:]:
    layer.trainable = True

# Compile model
model_mobilenet.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy", top_2_accuracy, top_3_accuracy]
)



from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

from tensorflow.keras.callbacks import TensorBoard
import datetime


checkpoint_cb = ModelCheckpoint(
    filepath='models/model_mobilenet_tl/best_model.h5',             # O .keras para el nuevo formato
    monitor='val_accuracy',               # Métrica que decides monitorear
    save_best_only=True,                  # Guarda solo el mejor modelo
    mode='max',                           # Porque estás maximizando
    verbose=1
)

log_dir = "logs/fit/model_mobilenet_tl/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_cb = TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,            # Si quieres histogramas de activaciones
    write_graph=True,            # Guarda el grafo del modelo
    write_images=True            # Guarda imágenes de pesos y outputs (opcional)
)

early_stop_cb = EarlyStopping(
    monitor='val_loss',
    patience=20,
    restore_best_weights=True,
    mode='min',
    verbose=1
)


history_efficient = model_mobilenet.fit(
    train_ds,
    validation_data=val_ds,
    epochs=500,
    callbacks=[early_stop_cb,  checkpoint_cb, tensorboard_cb]
)


import matplotlib.pyplot as plt

plt.title("Loss during training")
plt.plot(history_efficient.history['loss'], label='train_loss')
plt.plot(history_efficient.history['val_loss'], label='val_loss')
plt.legend()
plt.show()


plot_top3_predictions(model_mobilenet, val_ds, id2label, rows=4, cols=3)


evaluate_and_save_metrics(model_mobilenet, val_ds, id2label, output_folder="./models/model_mobilenet_tl/")



